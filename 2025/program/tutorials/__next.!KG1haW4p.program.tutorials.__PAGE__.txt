1:"$Sreact.fragment"
c:I[97367,["/2025/_next/static/chunks/ff1a16fafef87110.js","/2025/_next/static/chunks/b71c1cfea9076b4b.js"],"OutletBoundary"]
d:"$Sreact.suspense"
:HL["/2025/images/people/Yang-Cao.jpg","image"]
:HL["/2025/images/people/Blaise-Delattre.jpg","image"]
:HL["/2025/images/people/Jianzhong-Qi.jpg","image"]
:HL["/2025/images/people/Feng-Liu.jpeg","image"]
:HL["/2025/images/people/Zhe-Xue.jpg","image"]
2:T454,Deep neural networks face the critical challenge of adversarial vulnerability: imperceptible perturbations can drastically alter predictions. Empirical defenses, such as adversarial training, improve robustness against known attacks but often fail under stronger or adaptive adversaries. In contrast, certified robustness provides provable guarantees that a model’s prediction remains stable within a prescribed perturbation region. This tutorial introduces two complementary approaches: Lipschitz-constrained networks, which enforce global sensitivity bounds via spectral norm regularization, convex potentials, and contractive architectures, yielding deterministic certificates; and randomized smoothing, a probabilistic method that transforms any classifier into a certifiably robust model by adding Gaussian noise and averaging predictions. We will also highlight applications across natural language processing, datacentric AI, and foundation models, illustrating how certification principles extend to robustness against diverse perturbations, data quality issues, and large-scale multimodal systems.0:{"buildId":"yiNaxz0KEyLkAZ00krylK","rsc":["$","$1","c",{"children":[["$","div",null,{"children":[["$","h2",null,{"className":"text-3xl font-bold mb-2","children":"Tutorials"}],["$","div",null,{"className":"w-[4em] h-[5px] bg-gradient-to-r from-primary from-30% to-secondary mb-10"}],["$","article",null,{"className":"pb-10","children":[["$","section","1",{"children":[["$","p",null,{"className":"text-primary uppercase tracking-[0.2em] mb-2 font-semibold","children":["Tutorial ",1]}],["$","h3",null,{"className":"font-bold text-2xl mb-2 text-gray-600 leading-9","children":"Robust Certificates for Neural Networks"}],["$","ul",null,{"className":"text-sm mt-6 mb-6 list-disc list-inside","children":[["$","li",null,{"children":[["$","span",null,{"className":"font-semibold inline md:min-w-30 md:inline-block","children":"Venue:"}]," ","Bali (in-person)"]}],"$undefined",["$","li",null,{"children":[["$","span",null,{"className":"font-semibold inline md:min-w-30 md:inline-block","children":"Bali Time:"}]," ","Fri, 5 Dec 2025 13:30 - 14:30 WITA (UTC+8)"]}]]}],["$","p",null,{"className":"","children":"$2"}],["$","div",null,{"className":"flex items-center space-x-2 mt-6","children":[["$","div",null,{"className":"w-0 h-0 border-t-[0.6em] border-t-transparent border-l-[1em] border-gray-600 border-b-[0.6em] border-b-transparent"}],["$","h3",null,{"className":"font-bold text-xl text-gray-600","children":["Speaker","s"]}]]}],["$","div",null,{"className":"flex flex-col","children":[["$","div","0",{"className":"bg-gray-100 mt-4 rounded-xl min-w-[20rem]","children":[["$","a",null,{"href":"https://yangcao888.github.io/","target":"_blank","rel":"noopener","className":"flex items-center mb-2 mr-5 w-fit float-start border-gray-100 border-2 rounded-tl-xl","children":["$","img",null,{"src":"/2025/images/people/Yang-Cao.jpg","alt":"Yang Cao","className":"w-32 h-32 rounded-tl-xl object-cover shadow-lg"}]}],["$","a",null,{"href":"https://yangcao888.github.io/","target":"_blank","rel":"noopener","className":"flex flex-col pt-5 pb-2 hover:text-primary transition-colors duration-300","children":[["$","h4",null,{"className":"font-bold text-[1.2em]","children":"Yang Cao"}],["$","p",null,{"className":"text-gray-500 font-medium text-[0.95em]","children":"Institute of Science Tokyo"}]]}],["$","p",null,{"className":"text-gray-600 text-[0.9em] px-4 pb-4","children":[["$","a",null,{"href":"https://yangcao888.github.io/","target":"_blank","rel":"noopener","className":"hover:text-primary transition-colors duration-300","children":"Yang Cao"}]," ","is an Associate Professor at the Department of Computer Science, Institute of Science Tokyo (Science Tokyo, formerly Tokyo Tech), and directing the Trustworthy Data Science and AI (TDSAI) Lab. He is passionate about studying and teaching on algorithmic trustworthiness in data science and AI. Two of his papers on data privacy were selected as best paper finalists in top- tier conferences IEEE ICDE 2017 and ICME 2020. He was a recipient of the IEEE Computer Society Japan Chapter Young Author Award 2019, Database Society of Japan Kambayashi Young Researcher Award 2021. His research projects were/are supported by JSPS, JST, MSRA, KDDI, LINE, WeBank, etc."]}]]}],"$L3"]}],"$L4"]}],"$L5","$L6","$L7"]}]]}],null,"$L8"]}],"loading":null,"isPartial":false}
3:["$","div","1",{"className":"bg-gray-100 mt-4 rounded-xl min-w-[20rem]","children":[["$","a",null,{"href":"https://www.lamsade.dauphine.fr/~bdelattre/","target":"_blank","rel":"noopener","className":"flex items-center mb-2 mr-5 w-fit float-start border-gray-100 border-2 rounded-tl-xl","children":["$","img",null,{"src":"/2025/images/people/Blaise-Delattre.jpg","alt":"Blaise Delattre","className":"w-32 h-32 rounded-tl-xl object-cover shadow-lg"}]}],["$","a",null,{"href":"https://www.lamsade.dauphine.fr/~bdelattre/","target":"_blank","rel":"noopener","className":"flex flex-col pt-5 pb-2 hover:text-primary transition-colors duration-300","children":[["$","h4",null,{"className":"font-bold text-[1.2em]","children":"Blaise Delattre"}],["$","p",null,{"className":"text-gray-500 font-medium text-[0.95em]","children":"Institute of Science Tokyo"}]]}],["$","p",null,{"className":"text-gray-600 text-[0.9em] px-4 pb-4","children":[["$","a",null,{"href":"https://www.lamsade.dauphine.fr/~bdelattre/","target":"_blank","rel":"noopener","className":"hover:text-primary transition-colors duration-300","children":"Blaise Delattre"}]," ","is a Postdoctoral Researcher in the TDSAI Lab, working with Prof. Yang Cao. He received his PhD in Computer Science from PSL University, where his research focused on certified adversarial robustness of deep neural networks, with contributions on Lipschitz-constrained architectures and randomized smoothing. His current interests focus on robustness and reliability of large language, vision–language, and other foundation models."]}]]}]
4:["$","div",null,{"className":"w-[5.9em] h-[5px] bg-gradient-to-r from-primary from-30% to-secondary mt-14 mb-6"}]
5:["$","section","2",{"children":[["$","p",null,{"className":"text-primary uppercase tracking-[0.2em] mb-2 font-semibold","children":["Tutorial ",2]}],["$","h3",null,{"className":"font-bold text-2xl mb-2 text-gray-600 leading-9","children":"Question Answering over Knowledge Bases in the Era of Large Language Models"}],["$","ul",null,{"className":"text-sm mt-6 mb-6 list-disc list-inside","children":[["$","li",null,{"children":[["$","span",null,{"className":"font-semibold inline md:min-w-30 md:inline-block","children":"Venue:"}]," ","Sydney (in-person), Bali (broadcast)"]}],["$","li",null,{"children":[["$","span",null,{"className":"font-semibold inline md:min-w-30 md:inline-block","children":"Sydney Time:"}]," ","Sat, 6 Dec 2025 13:00 - 14:00 AEDT (UTC+11)"]}],["$","li",null,{"children":[["$","span",null,{"className":"font-semibold inline md:min-w-30 md:inline-block","children":"Bali Time:"}]," ","Sat, 6 Dec 2025 10:00 - 11:00 WITA (UTC+8)"]}]]}],["$","p",null,{"className":"","children":"Knowledge Base Question Answering (KBQA) has emerged as a crucial paradigm for providing accurate, explainable, and domain-specific answers to natural language queries. With the rapid advancement of Large Language Models (LLMs), QA systems can leverage their powerful language understanding and generation capabilities. However, LLMs often struggle with hallucination, static knowledge, and limited interpretability. Integrating structured Knowledge Bases (KBs) addresses these limitations by providing explicit, updatable, and domain-specific factual knowledge. This tutorial provides an overview of KBQA in the era of LLMs. We introduce fundamental concepts of KBQA, discuss the strengths and limitations of LLMs and KBs, and survey state-of-the-art methods, including retrieval-augmented generation and knowledge integration approaches. Practical considerations, applications, and limitations are highlighted throughout."}],["$","div",null,{"className":"flex items-center space-x-2 mt-6","children":[["$","div",null,{"className":"w-0 h-0 border-t-[0.6em] border-t-transparent border-l-[1em] border-gray-600 border-b-[0.6em] border-b-transparent"}],["$","h3",null,{"className":"font-bold text-xl text-gray-600","children":["Speaker",""]}]]}],["$","div",null,{"className":"flex flex-col","children":[["$","div","0",{"className":"bg-gray-100 mt-4 rounded-xl min-w-[20rem]","children":[["$","a",null,{"href":"https://jianzhongqi.github.io/","target":"_blank","rel":"noopener","className":"flex items-center mb-2 mr-5 w-fit float-start border-gray-100 border-2 rounded-tl-xl","children":["$","img",null,{"src":"/2025/images/people/Jianzhong-Qi.jpg","alt":"Jianzhong Qi","className":"w-32 h-32 rounded-tl-xl object-cover shadow-lg"}]}],["$","a",null,{"href":"https://jianzhongqi.github.io/","target":"_blank","rel":"noopener","className":"flex flex-col pt-5 pb-2 hover:text-primary transition-colors duration-300","children":[["$","h4",null,{"className":"font-bold text-[1.2em]","children":"Jianzhong Qi"}],["$","p",null,{"className":"text-gray-500 font-medium text-[0.95em]","children":"The University of Melbourne"}]]}],["$","p",null,{"className":"text-gray-600 text-[0.9em] px-4 pb-4","children":[["$","a",null,{"href":"https://jianzhongqi.github.io/","target":"_blank","rel":"noopener","className":"hover:text-primary transition-colors duration-300","children":"Jianzhong Qi"}]," ","is an Associate Professor at The University of Melbourne and an ARC Future Fellow. His research concerns fundamental algorithms for management of and knowledge discovery from structured and semi-structured data. He has served as a PC Chair for the Australasian Database  Conference in 2020, and he has served as an Area Chair, Senior PC member, and PC member for top machine learning and database venues such as ICML, NeurIPS, ICLR, SIGMOD, ICDE and WWW."]}]]}]]}],"$L9"]}]
a:T445,The goal of this tutorial is to provide machine learning researchers and practitioners with a clear guideline for adapting Foundation Models in the context of parameter-efficient fine-tuning (PEFT). This tutorial moves beyond a simple catalog of PEFT techniques to introduce Neural Network Reprogrammability as a unifying framework that explains how and why modern PEFT methods work. The audience will learn to view techniques, e.g., prompt tuning, in-context learning, and model reprogramming, not as isolated methodologies, but as principled instances of a shared underlying idea: repurposing a fixed pre-trained model by strategically manipulating information at its interfaces. Attendees will walk away with a structured understanding of the adaptation lifecycle: from input manipulation to output alignment. The tutorial will synthesize existing methodologies and practical applications with a cohesive principle, enabling attendees to better analyze, choose, and design adaptation strategies for their own projects, without incurring substantial costs when fine-tuning Foundation Models.6:["$","section","3",{"children":[["$","p",null,{"className":"text-primary uppercase tracking-[0.2em] mb-2 font-semibold","children":["Tutorial ",3]}],["$","h3",null,{"className":"font-bold text-2xl mb-2 text-gray-600 leading-9","children":"Neural Network Reprogrammability: A Unified Framework for Parameter-Efficient Foundation Model Adaptation"}],["$","ul",null,{"className":"text-sm mt-6 mb-6 list-disc list-inside","children":[["$","li",null,{"children":[["$","span",null,{"className":"font-semibold inline md:min-w-30 md:inline-block","children":"Venue:"}]," ","Bali (in-person), Sydney (broadcast)"]}],["$","li",null,{"children":[["$","span",null,{"className":"font-semibold inline md:min-w-30 md:inline-block","children":"Sydney Time:"}]," ","Sat, 6 Dec 2025 14:00 - 15:00 AEDT (UTC+11)"]}],["$","li",null,{"children":[["$","span",null,{"className":"font-semibold inline md:min-w-30 md:inline-block","children":"Bali Time:"}]," ","Sat, 6 Dec 2025 11:00 - 12:00 WITA (UTC+8)"]}]]}],["$","p",null,{"className":"","children":"$a"}],["$","div",null,{"className":"flex items-center space-x-2 mt-6","children":[["$","div",null,{"className":"w-0 h-0 border-t-[0.6em] border-t-transparent border-l-[1em] border-gray-600 border-b-[0.6em] border-b-transparent"}],["$","h3",null,{"className":"font-bold text-xl text-gray-600","children":["Speaker",""]}]]}],["$","div",null,{"className":"flex flex-col","children":[["$","div","0",{"className":"bg-gray-100 mt-4 rounded-xl min-w-[20rem]","children":[["$","a",null,{"href":"https://fengliu90.github.io/","target":"_blank","rel":"noopener","className":"flex items-center mb-2 mr-5 w-fit float-start border-gray-100 border-2 rounded-tl-xl","children":["$","img",null,{"src":"/2025/images/people/Feng-Liu.jpeg","alt":"Feng Liu","className":"w-32 h-32 rounded-tl-xl object-cover shadow-lg"}]}],["$","a",null,{"href":"https://fengliu90.github.io/","target":"_blank","rel":"noopener","className":"flex flex-col pt-5 pb-2 hover:text-primary transition-colors duration-300","children":[["$","h4",null,{"className":"font-bold text-[1.2em]","children":"Feng Liu"}],["$","p",null,{"className":"text-gray-500 font-medium text-[0.95em]","children":"The University of Melbourne"}]]}],["$","p",null,{"className":"text-gray-600 text-[0.9em] px-4 pb-4","children":[["$","a",null,{"href":"https://fengliu90.github.io/","target":"_blank","rel":"noopener","className":"hover:text-primary transition-colors duration-300","children":"Feng Liu"}]," ","is a Senior Lecturer in Machine Learning and ARC DECRA Fellow at The University of Melbourne, where he directs the Trustworthy Machine Learning and Reasoning Lab. He is also a Visiting Scientist at RIKEN AIP. His research focuses on hypothesis testing and trustworthy machine learning. He has served as an area chair for ICML, NeurIPS, ICLR, and AISTATS, and as an editor or action editor for several leading journals. His work has been recognized with the NeurIPS 2022 Outstanding Paper Award and multiple Outstanding Reviewer Awards."]}]]}]]}],"$Lb"]}]
7:["$","section","4",{"children":[["$","p",null,{"className":"text-primary uppercase tracking-[0.2em] mb-2 font-semibold","children":["Tutorial ",4]}],["$","h3",null,{"className":"font-bold text-2xl mb-2 text-gray-600 leading-9","children":"Optimizing Quality and Efficiency in Federated Learning"}],["$","ul",null,{"className":"text-sm mt-6 mb-6 list-disc list-inside","children":[["$","li",null,{"children":[["$","span",null,{"className":"font-semibold inline md:min-w-30 md:inline-block","children":"Venue:"}]," ","Bali (in-person), Sydney (broadcast)"]}],["$","li",null,{"children":[["$","span",null,{"className":"font-semibold inline md:min-w-30 md:inline-block","children":"Sydney Time:"}]," ","Sat, 6 Dec 2025 15:30 - 16:30 AEDT (UTC+11)"]}],["$","li",null,{"children":[["$","span",null,{"className":"font-semibold inline md:min-w-30 md:inline-block","children":"Bali Time:"}]," ","Sat, 6 Dec 2025 12:30 - 13:30 WITA (UTC+8)"]}]]}],["$","p",null,{"className":"","children":"Federated Learning (FL) faces critical challenges in model generalization and  Non-IID data, which impact both the quality and efficiency of the learned models. This talk will present some advancements to address these issues. We first introduce a reinforcement federated domain generalization method, which uses a reinforcement learning agent to dynamically optimize feature representation for superior performance on unseen data domains. Next, we present a method to create privacy-preserving client data, effectively mitigating data heterogeneity. Finally, we describe a classifier debiased federated learning framework that directly corrects classifier bias from Non-IID data. Together, these approaches form a cohesive strategy for building more robust, accurate, and efficient FL systems."}],["$","div",null,{"className":"flex items-center space-x-2 mt-6","children":[["$","div",null,{"className":"w-0 h-0 border-t-[0.6em] border-t-transparent border-l-[1em] border-gray-600 border-b-[0.6em] border-b-transparent"}],["$","h3",null,{"className":"font-bold text-xl text-gray-600","children":["Speaker",""]}]]}],["$","div",null,{"className":"flex flex-col","children":[["$","div","0",{"className":"bg-gray-100 mt-4 rounded-xl min-w-[20rem]","children":[["$","a",null,{"href":"https://teacher.bupt.edu.cn/xuezhe/en/index.htm","target":"_blank","rel":"noopener","className":"flex items-center mb-2 mr-5 w-fit float-start border-gray-100 border-2 rounded-tl-xl","children":["$","img",null,{"src":"/2025/images/people/Zhe-Xue.jpg","alt":"Zhe Xue","className":"w-32 h-32 rounded-tl-xl object-cover shadow-lg"}]}],["$","a",null,{"href":"https://teacher.bupt.edu.cn/xuezhe/en/index.htm","target":"_blank","rel":"noopener","className":"flex flex-col pt-5 pb-2 hover:text-primary transition-colors duration-300","children":[["$","h4",null,{"className":"font-bold text-[1.2em]","children":"Zhe Xue"}],["$","p",null,{"className":"text-gray-500 font-medium text-[0.95em]","children":"Beijing University of Posts and Telecommunications"}]]}],["$","p",null,{"className":"text-gray-600 text-[0.9em] px-4 pb-4","children":[["$","a",null,{"href":"https://teacher.bupt.edu.cn/xuezhe/en/index.htm","target":"_blank","rel":"noopener","className":"hover:text-primary transition-colors duration-300","children":"Zhe Xue"}]," ","is a Professor at Beijing University of Posts and Telecommunications, specializing in data mining, multimodal learning, and federated learning. He has published many papers in top-tier journals and conferences such as ICML, NeurIPS, CVPR, AAAI, IJCAI, MM and WWW. His major honors include the IEEE CCIS Best Paper Award, IEEE BigComp Best Paper Award Runner-up, CCFAI Best Paper Award, and ChinaMM Best Student Paper Award."]}]]}]]}],false]}]
8:["$","$Lc",null,{"children":["$","$d",null,{"name":"Next.MetadataOutlet","children":"$@e"}]}]
9:["$","div",null,{"className":"w-[5.9em] h-[5px] bg-gradient-to-r from-primary from-30% to-secondary mt-14 mb-6"}]
b:["$","div",null,{"className":"w-[5.9em] h-[5px] bg-gradient-to-r from-primary from-30% to-secondary mt-14 mb-6"}]
e:null
